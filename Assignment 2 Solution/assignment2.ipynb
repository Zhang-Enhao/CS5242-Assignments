{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Welcome to CS 5242 **Assignment 2**\n",
    "\n",
    "ASSIGNMENT DEADLINE ⏰ : ** 3 March 2024**\n",
    "\n",
    "In this assignment, we have three parts:\n",
    "1. Implement some operations in CNNs from scratch *(2 Points)*\n",
    "2. Implement a simple CNN and train on MNIST using PyTorch  *(4 Points)*\n",
    "3. Implement a VGG network with PyTorch *(4 Points)*\n",
    "\n",
    "Colab is a hosted Jupyter notebook service that requires no setup to use, while providing access free of charge to computing resources including GPUs. In this semester, we will use Colab to run our experiments.\n",
    "1. Login Google Colab https://colab.research.google.com/\n",
    "2. In this assignment, We **need GPU** to training the CNN model. You may need to **choose GPU in Runtime -> Change runtime type -> Hardware accerator**\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "\n",
    "### **Grades Policy**\n",
    "\n",
    "We have 10 points for this homework. 15% off per day late, 0 scores if you submit it 7 days after the deadline.\n",
    "\n",
    "### **Cautions**\n",
    "\n",
    "**DO NOT** copy the code from the internet, e.g. GitHub.\n",
    "---\n",
    "\n",
    "**DO NOT** use any LLMs to write the code, e.g. ChatGPT.\n",
    "---\n",
    "\n",
    "### **Contact**\n",
    "\n",
    "Please feel free to contact us if you have any question about this homework or need any further information.\n",
    "\n",
    "Slack: Wangbo Zhao\n",
    "\n",
    "\n",
    "> If you have not join the slack group, you can click [here](https://join.slack.com/t/cs5242-2024spring/shared_invite/zt-2cw3jgqab-wFhoaIVa4RIX4fCZ_k~vjQ)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLeZHcOVBp4U"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Start by running the cell below to set up all required software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIgu_q2HBg-E",
    "outputId": "a9c2b260-0096-48c4-a458-e29f5c787b09",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:53:21.194453Z",
     "start_time": "2024-03-02T11:53:19.106710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (1.26.4)\r\n",
      "Requirement already satisfied: matplotlib in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (3.8.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from matplotlib) (4.49.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from matplotlib) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from matplotlib) (23.1)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from matplotlib) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "\u001B[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: torch in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (2.2.1)\r\n",
      "Requirement already satisfied: torchvision in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (0.17.1)\r\n",
      "Requirement already satisfied: filelock in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from torch) (2024.2.0)\r\n",
      "Requirement already satisfied: numpy in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/zhangenhao/miniconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "\u001B[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy matplotlib\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtXcchT5H2PH"
   },
   "source": [
    "Import the neccesary library and fix seed for Python, NumPy and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2Yodsn4H6CB",
    "outputId": "a333124d-a7a7-4be2-fcb4-dd52499d68bc",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:54:24.694464Z",
     "start_time": "2024-03-02T11:54:20.576313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x10944caf0>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTpFBLKSkKI0"
   },
   "source": [
    "Now let's setup the GPU environment. The colab provides a free GPU to use. Do as follows:\n",
    "\n",
    "- Runtime -> Change Runtime Type -> select `GPU` in Hardware accelerator\n",
    "- Click `connect` on the top-right\n",
    "\n",
    "After connecting to one GPU, you can check its status using `nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ES8KOxziiYky",
    "outputId": "0677f7da-7918-43d9-d8cf-4bdb257ff13d",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:54:27.523018Z",
     "start_time": "2024-03-02T11:54:27.399730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yrZD7DDExF4"
   },
   "source": [
    "Everything is ready, you can move on and ***Good Luck !*** 😃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm1f362vdRgF"
   },
   "source": [
    "## Implement some operations in CNNs from scratch\n",
    "\n",
    "In this section, you need to implement some operations commonly used in CNNs, including convolution and pooling.\n",
    "\n",
    "You need to compare the computational results of your implemented version with those of Pytorch, expecting that the error between the correct implementation and pytorch will be very small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NV3DXc2jgeg7"
   },
   "source": [
    "### Step 1\n",
    "Given a 32x32 pixels, 3 channels input, get a torch tensor with torch.randn()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3UxGJxTegq9O",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d14ee32c-abea-4825-a971-cb6cb1350a63",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:54:32.029310Z",
     "start_time": "2024-03-02T11:54:32.019844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.1258, -1.1524, -0.2506,  ...,  1.5863,  0.9463, -0.8437],\n",
      "          [-0.6136,  0.0316, -0.4927,  ..., -1.2341,  1.8197, -0.5515],\n",
      "          [-0.5692,  0.9200,  1.1108,  ..., -0.9565,  0.0335,  0.7101],\n",
      "          ...,\n",
      "          [ 1.0166,  1.2868,  2.0820,  ...,  0.8161, -0.5711, -0.1195],\n",
      "          [-0.4274,  0.8143, -1.4121,  ..., -0.1394, -0.3677, -0.4574],\n",
      "          [-1.2945,  0.7012, -1.9098,  ...,  0.5374,  1.0826, -1.7105]],\n",
      "\n",
      "         [[-1.0841, -0.1287, -0.6811,  ..., -0.9825,  0.7184,  0.4402],\n",
      "          [-0.5619,  0.6640, -2.1033,  ..., -0.7821, -2.1407,  0.3337],\n",
      "          [-1.1230,  0.6210, -0.8764,  ...,  0.9159,  0.2990,  0.1771],\n",
      "          ...,\n",
      "          [ 2.2746, -0.9119,  0.5105,  ...,  0.4876, -0.9265, -0.5748],\n",
      "          [ 0.7300, -0.9287,  0.1743,  ..., -0.7073, -0.8813, -0.5895],\n",
      "          [-0.8363, -1.8354,  0.4765,  ..., -0.3812, -1.6687,  1.0869]],\n",
      "\n",
      "         [[ 0.6657,  0.8847,  0.4671,  ...,  0.7709, -0.8416,  1.7962],\n",
      "          [ 0.1924, -0.1777,  0.3214,  ..., -1.1616, -0.5921,  0.7457],\n",
      "          [-1.1870, -0.8221,  0.6051,  ..., -0.1906,  0.2511,  1.3542],\n",
      "          ...,\n",
      "          [ 1.9324, -0.5826, -1.3121,  ...,  0.2871,  0.2620, -0.3582],\n",
      "          [ 2.8424, -0.6401, -0.5874,  ...,  0.4994, -1.5602,  1.1315],\n",
      "          [-0.0504,  0.5482, -1.2351,  ..., -0.6380, -1.1714, -0.8415]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -0.0779, -0.1979,  ...,  0.8558, -0.2028, -0.3681],\n",
      "          [-0.6534, -0.8922, -0.2236,  ...,  0.0599, -0.9237, -0.1253],\n",
      "          [-0.5604,  1.1514,  0.2366,  ..., -1.4330, -0.2642,  0.3111],\n",
      "          ...,\n",
      "          [ 1.2874,  0.9561, -1.9727,  ...,  1.0564, -0.1504,  0.7420],\n",
      "          [ 0.7272, -0.2612,  0.0124,  ..., -1.0294, -1.8653, -0.7406],\n",
      "          [ 0.9192, -0.5652, -0.7208,  ..., -3.0357, -1.7288,  0.6020]],\n",
      "\n",
      "         [[ 1.9476,  1.0077, -0.1007,  ..., -1.5322,  0.0142, -0.3296],\n",
      "          [ 0.7450, -0.5086, -0.5947,  ..., -0.1173, -0.6841,  0.5988],\n",
      "          [-0.2579, -1.0667, -0.7595,  ...,  1.4624, -0.0423, -1.2064],\n",
      "          ...,\n",
      "          [ 0.0664, -0.0293,  0.6167,  ...,  0.0214, -0.7314, -0.9386],\n",
      "          [-1.5552, -0.7902, -0.2326,  ...,  0.0857,  0.7980,  0.8385],\n",
      "          [-1.4122, -0.5026, -1.0330,  ..., -2.3026, -1.8670, -0.1782]],\n",
      "\n",
      "         [[ 0.3533, -0.1317, -1.6393,  ..., -1.5319, -0.4684,  0.2686],\n",
      "          [-1.1515, -1.8388, -2.2352,  ..., -2.1816, -0.7663, -0.1473],\n",
      "          [ 1.2842,  0.6412, -1.0515,  ..., -1.4651, -2.6220,  1.0813],\n",
      "          ...,\n",
      "          [-0.2785,  0.6627, -0.2764,  ..., -1.7816,  1.5874, -1.2078],\n",
      "          [-0.7160, -2.0093, -0.8437,  ..., -1.2968, -0.8429,  0.9080],\n",
      "          [-1.3241,  1.9042, -0.4646,  ...,  0.3333, -0.2713,  0.0072]]]])\n",
      "torch.Size([2, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "c = 3\n",
    "h = 32\n",
    "w = 32\n",
    "x = torch.randn(batch_size, c, h, w)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxnlbBnFw9wB"
   },
   "source": [
    "### Step 2\n",
    "We first implement these operations with Pytorch so that we can compare the computational results of our implemented version with those of original pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OLQGhRbJgpIZ",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:54:38.794692Z",
     "start_time": "2024-03-02T11:54:38.788770Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Build a max pooling layer torch_max_pool with Pytorch. The kernel size of the pooling is 2, the stride is 2, and there is not any padding.\n",
    "torch_max_pool = nn.MaxPool2d(kernel_size=2,\n",
    "                              stride=2,\n",
    "                              padding=0)\n",
    "\n",
    "# 2. Build a average pooling layer torch_avg_pool with Pytorch. The kernel size of the pooling is 2, the stride is 1. The padding should be set to 1.\n",
    "torch_avg_pool = nn.AvgPool2d(kernel_size=2,\n",
    "                              stride=1,\n",
    "                              padding=1)\n",
    "\n",
    "# 3.Build a 2D convolutional layer torch_conv with Pytorch. The kernel size of the convolution is 3. Stride is 1. The input channel and output channel should be set to 3 and 64, respectively. We use zero padding to keep the spatial size of the output feature.\n",
    "torch_conv = nn.Conv2d(in_channels=3,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1)\n",
    "\n",
    "# 2D batchnorm with channel=3\n",
    "torch_norm = nn.BatchNorm2d(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PBzDAo2rgwmx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "755ae175-11f0-4ab5-ea3b-4f19dfa582fa",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:54:41.643882Z",
     "start_time": "2024-03-02T11:54:41.635026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 16, 16])\n",
      "torch.Size([2, 3, 33, 33])\n",
      "torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "torch_max_pool_out = torch_max_pool(x)\n",
    "print(torch_max_pool_out.shape)\n",
    "\n",
    "torch_avg_pool_out = torch_avg_pool(x)\n",
    "print(torch_avg_pool_out.shape)\n",
    "\n",
    "torch_conv_out = torch_conv(x)\n",
    "print(torch_conv_out.shape)\n",
    "\n",
    "torch_norm_out = torch_norm(x)\n",
    "print(torch_norm_out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6YBRP6Qgylg"
   },
   "source": [
    "### Step 3\n",
    "\n",
    "Implement these operations from scratch. Output your tensors as \"my_xxx_out\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CsO5I40fgzWY",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:54:44.898011Z",
     "start_time": "2024-03-02T11:54:44.890928Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_max_pool(x, kernel_size, stride, padding):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
    "        kernel_size: size of the window to take a max over,\n",
    "        stride: stride of the window,\n",
    "        padding: implicit zero padding to be added on both sides,\n",
    "\n",
    "    Return:\n",
    "        y: torch tensor of size (N, C_out, H_out, W_out).\n",
    "    \"\"\"\n",
    "\n",
    "    y = None\n",
    "    # === Complete the code (0.5')\n",
    "    N, C_in, H_in, W_in = x.shape\n",
    "    # Calculate output dimensions\n",
    "    H_out = (H_in + 2 * padding - kernel_size) // stride + 1\n",
    "    W_out = (W_in + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "    # Apply padding\n",
    "    if padding > 0:\n",
    "        x_padded = torch.nn.functional.pad(x, (padding, padding, padding, padding), \"constant\", 0)\n",
    "    else:\n",
    "        x_padded = x\n",
    "\n",
    "    # Initialize output tensor\n",
    "    y = torch.zeros((N, C_in, H_out, W_out), dtype=x.dtype)\n",
    "\n",
    "    # Apply max pooling\n",
    "    for n in range(N):\n",
    "        for c in range(C_in):\n",
    "            for h in range(H_out):\n",
    "                for w in range(W_out):\n",
    "                    h_start = h * stride\n",
    "                    w_start = w * stride\n",
    "                    h_end = h_start + kernel_size\n",
    "                    w_end = w_start + kernel_size\n",
    "\n",
    "                    window = x_padded[n, c, h_start:h_end, w_start:w_end]\n",
    "                    y[n, c, h, w] = torch.max(window)\n",
    "\n",
    "    # === Complete the code\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nGbn6oQcg4pM",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:54:47.971279Z",
     "start_time": "2024-03-02T11:54:47.965157Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_avg_pool(x, kernel_size, stride, padding):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
    "        kernel_size: size of the window,\n",
    "        stride: stride of the window,\n",
    "        padding: implicit zero padding to be added on both sides,\n",
    "\n",
    "    Return:\n",
    "        y: torch tensor of size (N, C_out, H_out, W_out).\n",
    "    \"\"\"\n",
    "\n",
    "    y = None\n",
    "    # === Complete the code (0.5')\n",
    "    N, C_in, H_in, W_in = x.shape\n",
    "    # Calculate output dimensions\n",
    "    H_out = (H_in + 2 * padding - kernel_size) // stride + 1\n",
    "    W_out = (W_in + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "    # Apply padding\n",
    "    if padding > 0:\n",
    "        x_padded = torch.nn.functional.pad(x, (padding, padding, padding, padding), \"constant\", 0)\n",
    "    else:\n",
    "        x_padded = x\n",
    "\n",
    "    # Initialize output tensor\n",
    "    y = torch.zeros((N, C_in, H_out, W_out), dtype=x.dtype)\n",
    "\n",
    "    # Apply average pooling\n",
    "    for n in range(N):\n",
    "        for c in range(C_in):\n",
    "            for h in range(H_out):\n",
    "                for w in range(W_out):\n",
    "                    h_start = h * stride\n",
    "                    w_start = w * stride\n",
    "                    h_end = h_start + kernel_size\n",
    "                    w_end = w_start + kernel_size\n",
    "\n",
    "                    window = x_padded[n, c, h_start:h_end, w_start:w_end]\n",
    "                    y[n, c, h, w] = torch.mean(window)\n",
    "\n",
    "    # === Complete the code\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9gsDytvKg5c1",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:54:50.717743Z",
     "start_time": "2024-03-02T11:54:50.711409Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_conv(x, in_channels, out_channels, kernel_size, stride, padding, weight, bias):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
    "        in_channels: number of channels in the input image, it is C_in;\n",
    "        out_channels: number of channels produced by the convolution;\n",
    "        kernel_size: size of onvolving kernel,\n",
    "        stride: stride of the convolution,\n",
    "        padding: implicit zero padding to be added on both sides of each dimension,\n",
    "\n",
    "    Return:\n",
    "        y: torch tensor of size (N, C_out, H_out, W_out)\n",
    "    \"\"\"\n",
    "\n",
    "    y = None\n",
    "    # === Complete the code (0.5')\n",
    "    N, C_in, H_in, W_in = x.shape\n",
    "    # Calculate output dimensions\n",
    "    H_out = (H_in + 2 * padding - kernel_size) // stride + 1\n",
    "    W_out = (W_in + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "    # Apply padding\n",
    "    x_padded = torch.nn.functional.pad(x, (padding, padding, padding, padding), \"constant\", 0)\n",
    "\n",
    "    # Initialize output tensor\n",
    "    y = torch.zeros((N, out_channels, H_out, W_out), dtype=x.dtype)\n",
    "\n",
    "    # Apply convolution\n",
    "    for n in range(N):\n",
    "        for c_out in range(out_channels):\n",
    "            for h in range(H_out):\n",
    "                for w in range(W_out):\n",
    "                    h_start = h * stride\n",
    "                    w_start = w * stride\n",
    "                    h_end = h_start + kernel_size\n",
    "                    w_end = w_start + kernel_size\n",
    "\n",
    "                    x_slice = x_padded[n, :, h_start:h_end, w_start:w_end]\n",
    "                    y[n, c_out, h, w] = torch.sum(x_slice * weight[c_out], dim=(0, 1, 2)) + bias[c_out]\n",
    "    # === Complete the code\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8sX0oRyTg-m6",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:54:53.956547Z",
     "start_time": "2024-03-02T11:54:53.952403Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_batchnorm(x, num_features, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: torch tensor with size (N, C, H, W),\n",
    "        num_features: number of features in the input tensor, it is C;\n",
    "        eps: a value added to the denominator for numerical stability. Default: 1e-5\n",
    "\n",
    "    Return:\n",
    "        y: torch tensor of size (N, C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    y = torch.empty_like(x)\n",
    "    # === Complete the code (0.5')\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # Calculate mean and variance\n",
    "    mean = x.mean(dim=(0, 2, 3), keepdim=True)\n",
    "    var = x.var(dim=(0, 2, 3), keepdim=True, unbiased=False)\n",
    "\n",
    "    # Normalize\n",
    "    y = (x - mean) / torch.sqrt(var + eps)\n",
    "\n",
    "    # === Complete the code\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dMnKzeVuhGxu",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:55:10.082933Z",
     "start_time": "2024-03-02T11:55:08.069748Z"
    }
   },
   "outputs": [],
   "source": [
    "my_max_pool_out = my_max_pool(x, kernel_size=2, stride=2, padding=0)\n",
    "my_avg_pool_out = my_avg_pool(x, kernel_size=2, stride=1, padding=1)\n",
    "my_conv_out = my_conv(x,\n",
    "                      in_channels=3,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      weight=torch_conv.weight,\n",
    "                      bias=torch_conv.bias)\n",
    "my_norm_out = my_batchnorm(x, num_features=3, eps=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jO-EHT7wm7Dk"
   },
   "source": [
    "### Step 4\n",
    "\n",
    "Compare and show that \"torch_xxx_out\" and \"my_xxx_out\" are equal up to small numerical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eXnNfKKJhOAi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f0188c49-5a0a-47e4-a2df-7092bbd954b8",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:55:11.478305Z",
     "start_time": "2024-03-02T11:55:11.472877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(4.2238e-16)\n",
      "tensor(4.8772e-15, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8154e-15, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(F.mse_loss(my_max_pool_out, torch_max_pool_out))\n",
    "print(F.mse_loss(my_avg_pool_out, torch_avg_pool_out))\n",
    "print(F.mse_loss(my_conv_out, torch_conv_out))\n",
    "print(F.mse_loss(my_norm_out, torch_norm_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJFZB_A7ddrC"
   },
   "source": [
    "## Implement a simple CNN and train it on MNIST using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV14tSG1I1fe"
   },
   "source": [
    "### Step 1\n",
    "Create datasets. The MNIST data set is composed of handwritten digit images and digit labels from 0 to 9. It consists of 60,000 training samples and 10,000 test samples. Each sample is a 28 * 28 pixel grayscale handwritten digit image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YwFXqatBI1fe",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9a2c45c7-710e-48ea-fd4e-267e955d925f",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:55:36.157052Z",
     "start_time": "2024-03-02T11:55:36.131003Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = 'FashionMNIST/',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root = 'FashionMNIST/',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIkZgfQLI1fe"
   },
   "source": [
    "### Step 2\n",
    "Create the model.\n",
    "You can build a simple convolutional neural network to conduct the classification. You may refine the architecture based on the accuracy. You can also try different learning rates.\n",
    "**The test accuracy should achieve 85%.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wMb5ePibI1ff",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:55:44.122494Z",
     "start_time": "2024-03-02T11:55:44.113204Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        # Define the network layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(in_features=64 * 7 * 7, out_features=128)  # Assuming input images are 28x28\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)  # Assuming 10 classes\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.pool1(self.relu1(self.conv1(input)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor for the fully connected layer\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        t = self.fc2(x)\n",
    "\n",
    "        return t\n",
    "\n",
    "network = Network()\n",
    "if torch.cuda.is_available():\n",
    "    network = network.cuda()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlbzK7M-I1ff"
   },
   "source": [
    "### Step 3\n",
    "\n",
    "Build the train and test loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "O3CezewoI1ff",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:58:48.945293Z",
     "start_time": "2024-03-02T11:55:47.828826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0  ,  Loss:249.585365  , Train Accuracy:84.935000 \n",
      "Epoch:1  ,  Loss:173.049925  , Train Accuracy:89.330000 \n",
      "Epoch:2  ,  Loss:154.921687  , Train Accuracy:90.358333 \n",
      "Epoch:3  ,  Loss:144.079812  , Train Accuracy:91.001667 \n",
      "Epoch:4  ,  Loss:134.901535  , Train Accuracy:91.515000 \n",
      "Epoch:5  ,  Loss:129.654091  , Train Accuracy:91.803333 \n",
      "Epoch:6  ,  Loss:122.124460  , Train Accuracy:92.360000 \n",
      "Epoch:7  ,  Loss:117.382260  , Train Accuracy:92.713333 \n",
      "Epoch:8  ,  Loss:116.227659  , Train Accuracy:92.713333 \n",
      "Epoch:9  ,  Loss:109.195896  , Train Accuracy:93.123333 \n",
      "Test Accuracy:  88.88000000000001\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _,prelabels=torch.max(preds,dim=1)\n",
    "        total_correct += (prelabels==labels).sum().item()\n",
    "    accuracy = total_correct/len(train_set)\n",
    "    print(\"Epoch:%d  ,  Loss:%f  , Train Accuracy:%f \"%(epoch, total_loss, accuracy * 100))\n",
    "\n",
    "\n",
    "correct=0\n",
    "total=0\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        imgs,labels=batch\n",
    "        if torch.cuda.is_available():\n",
    "            imgs = imgs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        preds=network(imgs)\n",
    "        _,prelabels=torch.max(preds,dim=1)\n",
    "        #print(prelabels.size())\n",
    "        total=total+labels.size(0)\n",
    "        correct=correct+int((prelabels==labels).sum())\n",
    "    #print(total)\n",
    "    accuracy=correct / total\n",
    "    print(\"Test Accuracy: \", accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3DJiFHrI1ff"
   },
   "source": [
    "# Implement a VGG network with PyTorch\n",
    "VGG is a type of CNN (Convolutional Neural Network) that was considered to be one of the best computer vision models in 2015.\n",
    "https://arxiv.org/abs/1409.1556\n",
    "\n",
    "Here is the configuration of the network from its paper. Now, you need to implement **Config C** it with Pytorch.\n",
    "\n",
    "![Alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3qayTtLfI1ff",
    "ExecuteTime": {
     "end_time": "2024-03-02T11:58:58.005903Z",
     "start_time": "2024-03-02T11:58:57.999123Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, num_classes=1000) -> None:\n",
    "        super().__init__()\n",
    "        # Define the convolutional blocks\n",
    "        # The configuration for Config C is:\n",
    "        # '64', '64', 'M', '128', '128', 'M', '256', '256', '256', 'M', '512', '512', '512', 'M', '512', '512', '512', 'M'\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Define the classifier (fully connected layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, image):\n",
    "        x = self.features(image)\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layers\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfckAk1RI1ff"
   },
   "source": [
    "Then, please calculate the number of parameters and FLOPs (Floating point operations) of **Config C**.\n",
    "You can only consider the FLOPs of the convolution and FC in **Config C**.\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(133638952, 24390336512)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_layers = [\n",
    "    (3, 64, 3, 1, 1),\n",
    "    (64, 64, 3, 1, 1),\n",
    "    'M',  # Adding 'M' to represent MaxPooling which affects the size but not FLOPs directly\n",
    "    (64, 128, 3, 1, 1),\n",
    "    (128, 128, 3, 1, 1),\n",
    "    'M',\n",
    "    (128, 256, 3, 1, 1),\n",
    "    (256, 256, 3, 1, 1),\n",
    "    (256, 256, 1, 1, 1),\n",
    "    'M',\n",
    "    (256, 512, 3, 1, 1),\n",
    "    (512, 512, 3, 1, 1),\n",
    "    (512, 512, 1, 1, 1),\n",
    "    'M',\n",
    "    (512, 512, 3, 1, 1),\n",
    "    (512, 512, 3, 1, 1),\n",
    "    (512, 512, 1, 1, 1),\n",
    "    'M',\n",
    "]\n",
    "\n",
    "fc_layers = [\n",
    "    (512 * 7 * 7, 4096),\n",
    "    (4096, 4096),\n",
    "    (4096, 1000),\n",
    "]\n",
    "\n",
    "# Initialize counts\n",
    "total_params = 0\n",
    "total_flops = 0\n",
    "\n",
    "# Image dimensions, starting with the input layer\n",
    "input_width, input_height = 224, 224\n",
    "\n",
    "# Convolutional layers\n",
    "for layer in model_layers:\n",
    "    if layer == 'M':  # MaxPooling, halving the dimensions\n",
    "        input_width /= 2\n",
    "        input_height /= 2\n",
    "    else:\n",
    "        in_channels, out_channels, kernel_size, stride, padding = layer\n",
    "        # Parameter count for Conv Layers: (kernel_size^2 * in_channels + 1) * out_channels\n",
    "        params = (kernel_size ** 2 * in_channels + 1) * out_channels\n",
    "        \n",
    "        # Output dimensions\n",
    "        output_width = int((input_width + 2*padding - kernel_size) / stride + 1)\n",
    "        output_height = int((input_height + 2*padding - kernel_size) / stride + 1)\n",
    "        \n",
    "        # FLOPs for Conv Layers: 2 * kernel_size^2 * in_channels * output_width * output_height * out_channels\n",
    "        flops = 2 * kernel_size ** 2 * in_channels * output_width * output_height * out_channels\n",
    "        \n",
    "        total_params += params\n",
    "        total_flops += flops\n",
    "        \n",
    "        # Update dimensions for next layer\n",
    "        input_width, input_height = output_width, output_height\n",
    "\n",
    "# Fully connected layers\n",
    "for in_features, out_features in fc_layers:\n",
    "    # Parameter count for FC layers: (in_features + 1) * out_features (including bias)\n",
    "    params = (in_features + 1) * out_features\n",
    "    # FLOPs for FC layers: 2 * in_features * out_features\n",
    "    flops = 2 * in_features * out_features\n",
    "    \n",
    "    total_params += params\n",
    "    total_flops += flops\n",
    "\n",
    "total_params, total_flops\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T06:07:11.288919Z",
     "start_time": "2024-03-03T06:07:11.278943Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Config C of the VGG network:\n",
    "The total number of parameters is approximately 134 million.\n",
    "The total number of floating point operations (FLOPs) for the convolutional and fully connected layers is approximately 2.4 billion."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
