{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXMX8yuAyl55"
   },
   "source": [
    "# Welcome to CS 5242 **Assignment 6**\n",
    "\n",
    "ASSIGNMENT DEADLINE â° : ** 18 April 2024** \n",
    "\n",
    "In this assignment, we learn how to adopt a parameter-efficient fine-tuning (PEFT) method to fine-tune a large model. The parameter-efficient fine-tuning mehtod is a very popular technique for large model training.\n",
    "\n",
    "we have three parts:\n",
    "1. Load pre-trained parameters correctly. 2 points\n",
    "2. Implement an adapter module and insert it into your model.  6 points\n",
    "3. Fine-tuning the model for 1 epoch. 2 points.\n",
    "\n",
    "Colab is a hosted Jupyter notebook service that requires no setup to use, while providing access free of charge to computing resources including GPUs. In this semester, we will use Colab to run our experiments.\n",
    "1. Login Google Colab https://colab.research.google.com/\n",
    "2. In this assignment, We **need GPU** to training the CNN model. You may need to **choose GPU in Runtime -> Change runtime type -> Hardware accerator**\n",
    "\n",
    "\n",
    "\n",
    "### **Grades Policy**\n",
    "\n",
    "We have 10 points for this homework. 15% off per day late, 0 scores if you submit it 7 days after the deadline.\n",
    "\n",
    "### **Cautions**\n",
    "\n",
    "**DO NOT** copy the code from the internet, e.g. GitHub.\n",
    "---\n",
    "\n",
    "**DO NOT** use any LLMs to write the code, e.g. ChatGPT.\n",
    "---\n",
    "\n",
    "### **Contact**\n",
    "\n",
    "Please feel free to contact us if you have any question about this homework or need any further information.\n",
    "\n",
    "Slack: Wangbo Zhao\n",
    "\n",
    "\n",
    "> If you have not join the slack group, you can click [here](https://join.slack.com/t/cs5242-2024spring/shared_invite/zt-2cw3jgqab-wFhoaIVa4RIX4fCZ_k~vjQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLeZHcOVBp4U"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Start by running the cell below to set up all required software."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIgu_q2HBg-E",
    "outputId": "1cf793d6-017c-480d-ccf7-bdafc8c4a4b0",
    "ExecuteTime": {
     "end_time": "2024-04-13T14:36:33.059164Z",
     "start_time": "2024-04-13T14:36:30.923692Z"
    }
   },
   "source": [
    "!pip install numpy matplotlib \n",
    "!pip install torch torchvision\n",
    "!pip install timm==0.9.16"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (1.26.4)\r\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.8.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.51.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (24.0)\r\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (10.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (0.17.2)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.13.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.3.1)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision) (10.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: timm==0.9.16 in ./.venv/lib/python3.12/site-packages (0.9.16)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from timm==0.9.16) (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (from timm==0.9.16) (0.17.2)\r\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from timm==0.9.16) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.12/site-packages (from timm==0.9.16) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.12/site-packages (from timm==0.9.16) (0.4.2)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm==0.9.16) (3.13.3)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm==0.9.16) (2024.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm==0.9.16) (24.0)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm==0.9.16) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm==0.9.16) (4.66.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm==0.9.16) (4.11.0)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch->timm==0.9.16) (1.12)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch->timm==0.9.16) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->timm==0.9.16) (3.1.3)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision->timm==0.9.16) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision->timm==0.9.16) (10.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->timm==0.9.16) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->timm==0.9.16) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->timm==0.9.16) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->timm==0.9.16) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->timm==0.9.16) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.12/site-packages (from sympy->torch->timm==0.9.16) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtXcchT5H2PH"
   },
   "source": [
    "Import the neccesary library and fix seed for Python, NumPy and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2Yodsn4H6CB",
    "outputId": "abeeb6fb-59b7-4318-eb50-5b54a949f6de",
    "ExecuteTime": {
     "end_time": "2024-04-13T14:36:34.545308Z",
     "start_time": "2024-04-13T14:36:33.060787Z"
    }
   },
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12c3e4330>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTpFBLKSkKI0"
   },
   "source": [
    "Now let's setup the GPU environment. The colab provides a free GPU to use. Do as follows:\n",
    "\n",
    "- Runtime -> Change Runtime Type -> select `GPU` in Hardware accelerator\n",
    "- Click `connect` on the top-right\n",
    "\n",
    "After connecting to one GPU, you can check its status using `nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ES8KOxziiYky",
    "outputId": "4e46e3bd-e922-4bc1-c7ca-bc6b4eae6d4b",
    "ExecuteTime": {
     "end_time": "2024-04-13T14:36:34.667406Z",
     "start_time": "2024-04-13T14:36:34.545835Z"
    }
   },
   "source": [
    "!nvidia-smi\n",
    "\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yrZD7DDExF4"
   },
   "source": [
    "Everything is ready, you can move on and ***Good Luck !*** ðŸ˜ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm1f362vdRgF"
   },
   "source": [
    "## Load parameters from pre-trained model.\n",
    "\n",
    "In this section, you need to load a checkpoint of the vision transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NV3DXc2jgeg7"
   },
   "source": [
    "\n",
    "Implement the code for ViT-Base. Do not worry, I have done it for you."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3UxGJxTegq9O",
    "ExecuteTime": {
     "end_time": "2024-04-13T14:36:34.900810Z",
     "start_time": "2024-04-13T14:36:34.668530Z"
    }
   },
   "source": [
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from typing import Any, Callable, Dict, Optional, Sequence, Set, Tuple, Type, Union, List\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "from torch.jit import Final\n",
    "from timm.layers import PatchEmbed, Mlp, DropPath, PatchDropout, trunc_normal_, use_fused_attn\n",
    "\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads=8,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            attn_drop=0.,\n",
    "            proj_drop=0.,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        if self.fused_attn:\n",
    "            x = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            mlp_layer=Mlp\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_norm=qk_norm,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = mlp_layer(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=proj_drop,\n",
    "        )\n",
    "        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n",
    "        - https://arxiv.org/abs/2010.11929\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size: Union[int, Tuple[int, int]] = 224,\n",
    "            patch_size: Union[int, Tuple[int, int]] = 16,\n",
    "            in_chans: int = 3,\n",
    "            num_classes: int = 1000,\n",
    "            global_pool: str = 'token',\n",
    "            embed_dim: int = 768,\n",
    "            depth: int = 12,\n",
    "            num_heads: int = 12,\n",
    "            mlp_ratio: float = 4.,\n",
    "            qkv_bias: bool = True,\n",
    "            qk_norm: bool = False,\n",
    "            init_values: Optional[float] = None,\n",
    "            class_token: bool = True,\n",
    "            no_embed_class: bool = False,\n",
    "            pre_norm: bool = False,\n",
    "            fc_norm: Optional[bool] = None,\n",
    "            drop_rate: float = 0.,\n",
    "            pos_drop_rate: float = 0.,\n",
    "            patch_drop_rate: float = 0.,\n",
    "            proj_drop_rate: float = 0.,\n",
    "            attn_drop_rate: float = 0.,\n",
    "            drop_path_rate: float = 0.,\n",
    "            weight_init: str = '',\n",
    "            embed_layer: Callable = PatchEmbed,\n",
    "            norm_layer: Optional[Callable] = None,\n",
    "            act_layer: Optional[Callable] = None,\n",
    "            block_fn: Callable = Block,\n",
    "            mlp_layer: Callable = Mlp\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size: Input image size.\n",
    "            patch_size: Patch size.\n",
    "            in_chans: Number of image input channels.\n",
    "            num_classes: Mumber of classes for classification head.\n",
    "            global_pool: Type of global pooling for final sequence (default: 'token').\n",
    "            embed_dim: Transformer embedding dimension.\n",
    "            depth: Depth of transformer.\n",
    "            num_heads: Number of attention heads.\n",
    "            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias: Enable bias for qkv projections if True.\n",
    "            init_values: Layer-scale init values (layer-scale enabled if not None).\n",
    "            class_token: Use class token.\n",
    "            fc_norm: Pre head norm after pool (instead of before), if None, enabled when global_pool == 'avg'.\n",
    "            drop_rate: Head dropout rate.\n",
    "            pos_drop_rate: Position embedding dropout rate.\n",
    "            attn_drop_rate: Attention dropout rate.\n",
    "            drop_path_rate: Stochastic depth rate.\n",
    "            weight_init: Weight initialization scheme.\n",
    "            embed_layer: Patch embedding layer.\n",
    "            norm_layer: Normalization layer.\n",
    "            act_layer: MLP activation layer.\n",
    "            block_fn: Transformer block layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert global_pool in ('', 'avg', 'token')\n",
    "        assert class_token or global_pool != 'token'\n",
    "        use_fc_norm = global_pool == 'avg' if fc_norm is None else fc_norm\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.global_pool = global_pool\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_prefix_tokens = 1 if class_token else 0\n",
    "        self.no_embed_class = no_embed_class\n",
    "        self.grad_checkpointing = False\n",
    "\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None\n",
    "        embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n",
    "        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n",
    "        if patch_drop_rate > 0:\n",
    "            self.patch_drop = PatchDropout(\n",
    "                patch_drop_rate,\n",
    "                num_prefix_tokens=self.num_prefix_tokens,\n",
    "            )\n",
    "        else:\n",
    "            self.patch_drop = nn.Identity()\n",
    "        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            block_fn(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_norm=qk_norm,\n",
    "                init_values=init_values,\n",
    "                proj_drop=proj_drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                mlp_layer=mlp_layer\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim) if not use_fc_norm else nn.Identity()\n",
    "\n",
    "        # Classifier Head\n",
    "        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()\n",
    "        self.head_drop = nn.Dropout(drop_rate)\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        if self.cls_token is not None:\n",
    "            nn.init.normal_(self.cls_token, std=1e-6)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif hasattr(m, '_init_weights'):\n",
    "            m._init_weights()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        if self.cls_token is not None:\n",
    "            x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        x = self.patch_drop(x)\n",
    "        x = self.norm_pre(x)\n",
    "\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x, pre_logits: bool = False):\n",
    "        if self.global_pool:\n",
    "            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n",
    "        x = self.fc_norm(x)\n",
    "        x = self.head_drop(x)\n",
    "        return x if pre_logits else self.head(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def convert_list_to_tensor(list_convert):\n",
    "    if len(list_convert):\n",
    "        result = torch.stack(list_convert, dim=1)\n",
    "    else :\n",
    "        result = None\n",
    "    return result \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def vit_base_patch16_224_in21k(**kwargs):\n",
    "    \"\"\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, **kwargs)\n",
    "    model = VisionTransformer(**model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxnlbBnFw9wB"
   },
   "source": [
    "\n",
    "Download pre-trained model from \"https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth\".\n",
    "Then load the parameters correctly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PBzDAo2rgwmx",
    "ExecuteTime": {
     "end_time": "2024-04-13T14:36:36.113281Z",
     "start_time": "2024-04-13T14:36:34.901926Z"
    }
   },
   "source": [
    "model = vit_base_patch16_224_in21k(num_classes=100)\n",
    "# checkpoint_model = torch.load(\"/path/jx_vit_base_patch16_224_in21k-e5005f0a.pth\") \n",
    "########## write code to load the checkpoint_model without error ##########\n",
    "checkpoint_model = torch.load(\"jx_vit_base_patch16_224_in21k-e5005f0a.pth\") \n",
    "del checkpoint_model['head.weight']\n",
    "del checkpoint_model['head.bias']\n",
    "########## write code to load the checkpoint_model without error ##########\n",
    "msg = model.load_state_dict(checkpoint_model, strict=False)\n",
    "print(msg)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['pre_logits.fc.bias', 'pre_logits.fc.weight'])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a adapter module. \n",
    "[AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition](https://proceedings.neurips.cc/paper_files/paper/2022/file/69e2f49ab0837b71b0e0cb7c555990f8-Paper-Conference.pdf)\n",
    "\n",
    "\n",
    "![Alt text](image.png)!\n",
    "The scale s is set to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T14:36:36.116759Z",
     "start_time": "2024-04-13T14:36:36.114242Z"
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_embd=768,\n",
    "                 down_size=8,\n",
    "                 dropout=0.0,\n",
    "                 adapter_scalar=\"0.1\"):\n",
    "        super().__init__()\n",
    "    ### implement the adapter module ####\n",
    "        self.down_proj = nn.Linear(n_embd, n_embd // down_size)  # Linear layer for downsizing\n",
    "        self.up_proj = nn.Linear(n_embd // down_size, n_embd)  # Linear layer for upsizing\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
    "        self.activation = nn.ReLU()  # Activation function\n",
    "        self.adapter_scalar = float(adapter_scalar)  # Adapter scaling factor\n",
    "\n",
    "    ### implement the adapter module ####\n",
    "    \n",
    "    def forward(self, x):\n",
    "    ### implement the adapter module ####\n",
    "        # Forward pass logic for the adapter\n",
    "        down_projected = self.down_proj(x)  # Pass through the downsizing linear layer\n",
    "        activated = self.activation(down_projected)  # Apply activation function\n",
    "        dropped = self.dropout(activated)  # Apply Dropout\n",
    "        up_projected = self.up_proj(dropped)  # Pass through the upsizing linear layer\n",
    "        # output = x + up_projected * self.adapter_scalar  # Add the adapter's output to the original input\n",
    "        # åœ¨ Adapter ç±»çš„ forward æ–¹æ³•ä¸­\n",
    "        scalar_tensor = torch.tensor(self.adapter_scalar, device=up_projected.device, dtype=up_projected.dtype)\n",
    "        output = x + up_projected * scalar_tensor\n",
    "\n",
    "\n",
    "    ### implement the adapter module ####\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the adapter module to your vision transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T14:36:36.184261Z",
     "start_time": "2024-04-13T14:36:36.117549Z"
    }
   },
   "source": [
    "import timm\n",
    "\n",
    "\n",
    "######## implement you code here###########\n",
    "def set_adapter(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, Block):\n",
    "            # Attempt to deduce the embedding dimension from the attention module's query/key/value linear layer\n",
    "            if hasattr(module.attn, 'qkv'):\n",
    "                n_embd = module.attn.qkv.in_features\n",
    "            else:\n",
    "                # Alternatively, try to get the embedding dimension from the first linear layer inside the block\n",
    "                linear_layers = [m for m in module.modules() if isinstance(m, nn.Linear)]\n",
    "                if linear_layers:\n",
    "                    n_embd = linear_layers[0].in_features\n",
    "                else:\n",
    "                    raise AttributeError(\"Unable to deduce the embedding dimension for the adapter module.\")\n",
    "\n",
    "            # Add the adapter module to the block\n",
    "            module.adapter = Adapter(n_embd=n_embd)\n",
    "\n",
    "            # Override the forward method to include the adapter\n",
    "            original_forward = module.forward\n",
    "\n",
    "            def forward_with_adapter(self, x, original_forward=original_forward):\n",
    "                x = original_forward(x)\n",
    "                x = self.adapter(x)\n",
    "                return x\n",
    "\n",
    "            module.forward = forward_with_adapter.__get__(module)\n",
    "\n",
    "######## implement you code here ###########\n",
    "\n",
    "\n",
    "\n",
    "set_adapter(model=model)\n",
    "\n",
    "# load pre-trained parameter agrain \n",
    "msg = model.load_state_dict(checkpoint_model, strict=False)\n",
    "print(msg)\n",
    "\n",
    "\n",
    "# freeze all but the head\n",
    "for name, p in model.named_parameters():\n",
    "    if name in msg.missing_keys:\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "for _, p in model.head.named_parameters():\n",
    "    p.requires_grad = True\n",
    "    \n",
    "n_parameters = 0\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        n_parameters = n_parameters + p.numel()\n",
    "\n",
    "\n",
    "print('number of tunable params (M): %.2f' % (n_parameters / 1.e6))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['blocks.0.adapter.down_proj.weight', 'blocks.0.adapter.down_proj.bias', 'blocks.0.adapter.up_proj.weight', 'blocks.0.adapter.up_proj.bias', 'blocks.1.adapter.down_proj.weight', 'blocks.1.adapter.down_proj.bias', 'blocks.1.adapter.up_proj.weight', 'blocks.1.adapter.up_proj.bias', 'blocks.2.adapter.down_proj.weight', 'blocks.2.adapter.down_proj.bias', 'blocks.2.adapter.up_proj.weight', 'blocks.2.adapter.up_proj.bias', 'blocks.3.adapter.down_proj.weight', 'blocks.3.adapter.down_proj.bias', 'blocks.3.adapter.up_proj.weight', 'blocks.3.adapter.up_proj.bias', 'blocks.4.adapter.down_proj.weight', 'blocks.4.adapter.down_proj.bias', 'blocks.4.adapter.up_proj.weight', 'blocks.4.adapter.up_proj.bias', 'blocks.5.adapter.down_proj.weight', 'blocks.5.adapter.down_proj.bias', 'blocks.5.adapter.up_proj.weight', 'blocks.5.adapter.up_proj.bias', 'blocks.6.adapter.down_proj.weight', 'blocks.6.adapter.down_proj.bias', 'blocks.6.adapter.up_proj.weight', 'blocks.6.adapter.up_proj.bias', 'blocks.7.adapter.down_proj.weight', 'blocks.7.adapter.down_proj.bias', 'blocks.7.adapter.up_proj.weight', 'blocks.7.adapter.up_proj.bias', 'blocks.8.adapter.down_proj.weight', 'blocks.8.adapter.down_proj.bias', 'blocks.8.adapter.up_proj.weight', 'blocks.8.adapter.up_proj.bias', 'blocks.9.adapter.down_proj.weight', 'blocks.9.adapter.down_proj.bias', 'blocks.9.adapter.up_proj.weight', 'blocks.9.adapter.up_proj.bias', 'blocks.10.adapter.down_proj.weight', 'blocks.10.adapter.down_proj.bias', 'blocks.10.adapter.up_proj.weight', 'blocks.10.adapter.up_proj.bias', 'blocks.11.adapter.down_proj.weight', 'blocks.11.adapter.down_proj.bias', 'blocks.11.adapter.up_proj.weight', 'blocks.11.adapter.up_proj.bias', 'head.weight', 'head.bias'], unexpected_keys=['pre_logits.fc.bias', 'pre_logits.fc.weight'])\n",
      "number of tunable params (M): 1.86\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJFZB_A7ddrC"
   },
   "source": [
    "## Parameter-efficient fine-tuning for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T14:36:37.445897Z",
     "start_time": "2024-04-13T14:36:36.184991Z"
    }
   },
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from timm.data.constants import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "import PIL.Image\n",
    "import math\n",
    "\n",
    "_mean = IMAGENET_INCEPTION_MEAN\n",
    "_std = IMAGENET_INCEPTION_STD\n",
    "\n",
    "class RandomResizedCrop(transforms.RandomResizedCrop):\n",
    "    \"\"\"\n",
    "    RandomResizedCrop for matching TF/TPU implementation: no for-loop is used.\n",
    "    This may lead to results different with torchvision's version.\n",
    "    Following BYOL's TF code:\n",
    "    https://github.com/deepmind/deepmind-research/blob/master/byol/utils/dataset.py#L206\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def get_params(img, scale, ratio):\n",
    "        assert isinstance(img, PIL.Image.Image)\n",
    "        # width, height = F._get_image_size(img)\n",
    "        width, height = img.width, img.height\n",
    "        area = height * width\n",
    "\n",
    "        target_area = area * torch.empty(1).uniform_(scale[0], scale[1]).item()\n",
    "        log_ratio = torch.log(torch.tensor(ratio))\n",
    "        aspect_ratio = torch.exp(\n",
    "            torch.empty(1).uniform_(log_ratio[0], log_ratio[1])\n",
    "        ).item()\n",
    "\n",
    "        w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "        h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "        w = min(w, width)\n",
    "        h = min(h, height)\n",
    "\n",
    "        i = torch.randint(0, height - h + 1, size=(1,)).item()\n",
    "        j = torch.randint(0, width - w + 1, size=(1,)).item()\n",
    "\n",
    "        return i, j, h, w\n",
    "    \n",
    "        \n",
    "train_set = torchvision.datasets.CIFAR100(\n",
    "    root = 'Cifar100',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        RandomResizedCrop(224, interpolation=3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=_mean, std=_std)])\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR100(\n",
    "    root = 'Cifar100',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256, interpolation=3),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=_mean, std=_std)])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "print(len(train_loader))\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "500\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T14:36:37.449264Z",
     "start_time": "2024-04-13T14:36:37.446612Z"
    }
   },
   "source": [
    "network = model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    network = network.cuda()\n",
    "    \n",
    "# you may aduject the learning rate or weight deacy here.\n",
    "\n",
    "######### complete the optimizer. ##################\n",
    "optimizer = optim.Adam(network.parameters(), weight_decay = 1e-5, lr=1e-3)\n",
    "######### complete the optimizer. ##################"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Build the train loop. You should finish the first epoch training!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tqdm\n",
    "for epoch in range(1):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in tqdm.tqdm(train_loader):  \n",
    "        images, labels = batch\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _,prelabels=torch.max(preds,dim=1)\n",
    "        total_correct += (prelabels==labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct/len(train_set)\n",
    "    print(\"Epoch:%d  ,  Loss:%f  , Train Accuracy:%f \"%(epoch, total_loss, accuracy * 100))\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
