{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\\# Welcome to CS 5242 **Homework 4**\n",
    "\n",
    "ASSIGNMENT DEADLINE ‚è∞ : **23:59 04 April 2024**\n",
    "\n",
    "In this assignment, we will delve into **different generation methods of GPT**. To be specific, you will practice different text generation methods using the powerful [OPT](https://arxiv.org/abs/2205.01068) language model based on [transformers](https://huggingface.co/docs/transformers/en/index) package. You will implement two generation techniques: greedy search, beam search and sampling.\n",
    "\n",
    "Helpful material: https://huggingface.co/blog/how-to-generate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:38:38.155225Z",
     "start_time": "2024-04-03T08:38:35.052072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized input: tensor([[    2,  1121,    42, 11717,     6,    52,    40, 33244,    88,   430,\n",
      "          2706,  6448,     9,   272, 10311,     4,   598,    28,  2167,     6,\n",
      "            47,    40]]), shape: torch.Size([1, 22])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the OPT model and tokenizer\n",
    "# To save memory, we only use opt-350m\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "prompt = \"In this assignment, we will delve into different generation methods of GPT. To be specific, you will\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(f\"tokenized input: {input_ids}, shape: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye5nZ6OVI6qg"
   },
   "source": [
    "### Task 1:  Greedy Search\n",
    "\n",
    "Greedy search is a simple and straightforward method for text generation. At each step, it selects the next token with the highest probability according to the language model's output. This approach is called \"greedy\" because it greedily chooses the most likely token at each time step without considering future consequences. Greedy search is fast but may lead to repetitive or monotonous results.\n",
    "\n",
    "Greedy search is a simple method where at each time step, the word with the highest probability is chosen as the next predicted word, until an end token is generated or the maximum length is reached.\n",
    "\n",
    "It can be represented by the formula:\n",
    "\n",
    "$ w_t = \\arg\\max P(w_t | w_{<t}) $\n",
    "\n",
    "Here, $ w_t $ is the predicted word at time step $ t $, and $ P(w_t | w_{<t}) $ is the probability of predicting $ w_t $ given the previous word sequence $ w_{<t} $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:38:44.177234Z",
     "start_time": "2024-04-03T08:38:38.156189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: In this assignment, we will delve into different generation methods of GPT. To be specific, you will\n",
      "generated_text: In this assignment, we will delve into different generation methods of GPT. To be specific, you will be working on a GPT-based system that is based on the following:\n",
      "\n",
      "Generation of a new GPT-based system\n",
      "\n",
      "Generation of a new GPT-based system\n",
      "\n",
      "Generation of a new GPT-based system\n",
      "\n",
      "Generation of a new GPT-based system\n"
     ]
    }
   ],
   "source": [
    "def greedy_search_generation(input_ids, model) -> str:\n",
    "    outputs = input_ids\n",
    "    \n",
    "    # =========================\n",
    "    # Your code starts here (3 points)\n",
    "    # outputs should be a torch Tensor of shape [1, n]\n",
    "    # where n is the length of the generated text\n",
    "    # and the elements are the token ids of the generated text\n",
    "    # \n",
    "    # You should not use any external library like transformers's .generate().\n",
    "    # Only torch is allowed.\n",
    "    # =========================\n",
    "    max_length = 64\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for _ in range(max_length):\n",
    "            predictions = model(outputs)[0]\n",
    "            # The shape of predictions is expected to be [batch_size, sequence_length, vocab_size]\n",
    "            next_token_id = predictions[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Check if the next token is an end-of-sequence token (e.g., tokenizer.eos_token_id)\n",
    "            # If so, break the loop\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            # Append the predicted token ID to the output sequence\n",
    "            outputs = torch.cat([outputs, next_token_id], dim=-1)\n",
    "            \n",
    "            # Optionally, you can add a condition to stop the loop after reaching a maximum sequence length\n",
    "\n",
    "    \n",
    "    # =========================\n",
    "    # Your code ends here\n",
    "    # =========================\n",
    "\n",
    "    return outputs\n",
    "\n",
    "generated_ids = greedy_search_generation(input_ids, model)[0]\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(f\"prompt: {prompt}\\ngenerated_text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQQ3k-DBFGEb"
   },
   "source": [
    "### Task 2: Beam Search\n",
    "\n",
    "\n",
    "Beam search is a more complex method that considers multiple candidate words rather than just a single word. At each time step, it keeps track of the top $ k $ most likely partial sequences (known as the beam width), and then expands these partial sequences based on their scores. Finally, it selects the sequence with the highest score from these expanded sequences. Beam search can provide more diverse results but might still result in repetitive outputs.\n",
    "\n",
    "Here's the basic algorithm for beam search:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Start with an initial input sequence (usually a special token indicating the beginning of a sequence).\n",
    "   - Initialize a set of beams, each containing the initial sequence and its probability score.\n",
    "\n",
    "2. **Expansion**:\n",
    "   - For each beam, generate the next token in the sequence using the GPT model.\n",
    "   - Calculate the conditional probability of each possible next token given the current partial sequence.\n",
    "   - Expand each beam by appending each possible next token to the partial sequence, along with its updated probability score.\n",
    "\n",
    "3. **Selection**:\n",
    "   - Select the top-k beams with the highest probability scores, where k is the beam width.\n",
    "   - Discard the remaining beams.\n",
    "\n",
    "4. **Termination**:\n",
    "   - If the generated token is an end-of-sequence token or a maximum sequence length is reached, terminate those beams.\n",
    "   - Repeat steps 2-4 until all remaining beams either terminate or reach the maximum sequence length.\n",
    "\n",
    "The beam search algorithm can be expressed mathematically as follows:\n",
    "\n",
    "- Let $ S_t^b $ denote the partial sequence for beam $ b $ at time step $ t $.\n",
    "- Let $ P(S_t^b) $ denote the probability of the partial sequence $ S_t^b $ up to time step $ t $.\n",
    "- Let $ P(w_t | S_t^b) $ denote the conditional probability of the next token $ w_t $ given the partial sequence $ S_t^b $ at time step $ t $.\n",
    "- Let $ K $ denote the beam width.\n",
    "\n",
    "The beam search algorithm can be summarized with the following formulas:\n",
    "\n",
    "1. **Initialization**:\n",
    "   $ S_1^1 = \\text{[BOS]} $\n",
    "   $ P(S_1^1) = 1 $\n",
    "\n",
    "2. **Expansion**:\n",
    "   $ P(w_t | S_t^b) = \\text{GPT\\_Model}(S_t^b, w_t) $\n",
    "   $ S_{t+1}^b = S_t^b \\cup \\{w_t\\} $\n",
    "   $ P(S_{t+1}^b) = P(S_t^b) \\times P(w_t | S_t^b) $\n",
    "\n",
    "3. **Selection**:\n",
    "   $ (S_{t+1}^{(1)}, ..., S_{t+1}^{(K)}) = \\text{Top-K}(S_{t+1}^1, ..., S_{t+1}^B, K) $\n",
    "\n",
    "4. **Termination**:\n",
    "   - Terminate beams that reach the maximum length or encounter an end-of-sequence token.\n",
    "\n",
    "This process continues until all terminated or maximum-length beams are obtained.\n",
    "\n",
    "In summary, beam search efficiently explores the space of possible sequences and provides a trade-off between exploration and exploitation, helping to find high-quality sequences in sequence generation tasks such as text generation with models like GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:39:08.782945Z",
     "start_time": "2024-04-03T08:38:44.178222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: In this assignment, we will delve into different generation methods of GPT. To be specific, you will\n",
      "generated_text: In this assignment, we will delve into different generation methods of GPT. To be specific, you will be asked to write an essay on a topic which is relevant to your field of study and you will also be asked to write an essay which you can edit later on if you wish to revise it later on or if you have any questions you would like answered.\n",
      "\n",
      "You will also be asked to write an essay on\n"
     ]
    }
   ],
   "source": [
    "def beam_search_generation(input_ids, model, beam_size) -> str:\n",
    "    outputs = input_ids\n",
    "    \n",
    "    # =========================\n",
    "    # Your code starts here (4 points)\n",
    "    # outputs should be a torch Tensor of shape [1, n]\n",
    "    # where n is the length of the generated text\n",
    "    # and the elements are the token ids of the generated text\n",
    "    # \n",
    "    # You should not use any external library like transformers's .generate().\n",
    "    # Only torch is allowed.\n",
    "    # =========================\n",
    "    model.eval()\n",
    "    max_length = 64\n",
    "    with torch.no_grad():\n",
    "        beams = [(outputs, 0)] \n",
    "        for _ in range(max_length):\n",
    "            new_beams = []\n",
    "            for beam, score in beams:\n",
    "                if beam[0, -1].item() == tokenizer.eos_token_id:\n",
    "                    new_beams.append((beam, score))\n",
    "                    continue\n",
    "                predictions = model(beam)[0][:, -1, :] \n",
    "                topk = predictions.topk(beam_size) \n",
    "                \n",
    "\n",
    "                for i in range(beam_size):\n",
    "                    next_token_id = topk.indices[0][i].unsqueeze(0).unsqueeze(0)\n",
    "                    log_prob = topk.values[0][i].item()\n",
    "                   \n",
    "                    new_beam = torch.cat([beam, next_token_id], dim=-1)\n",
    "                    new_score = score + log_prob  # Á¥ØËÆ°ÂæóÂàÜ\n",
    "                    new_beams.append((new_beam, new_score))\n",
    "            \n",
    "\n",
    "            new_beams.sort(key=lambda x: x[1], reverse=True) \n",
    "            beams = new_beams[:beam_size]\n",
    "        \n",
    "\n",
    "        outputs = beams[0][0]\n",
    "    \n",
    "    # =========================\n",
    "    # Your code ends here\n",
    "    # =========================\n",
    "\n",
    "    return outputs\n",
    "\n",
    "generated_ids = beam_search_generation(input_ids, model, beam_size=4)[0]\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(f\"prompt: {prompt}\\ngenerated_text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task3: Sampling\n",
    "\n",
    "Sampling method randomly selects the next word based on the probability distribution of words, instead of always choosing the word with the highest probability. This increases the diversity of generated outputs but lacks stability in results.\n",
    "\n",
    "It can be represented by the formula:\n",
    "\n",
    "$ w_t \\sim \\text{Top-}k\\left(P(w_t | w_{<t})\\right) $\n",
    "\n",
    "This formula introduces a top-k constraint to the sampling process, where instead of sampling from the entire probability distribution $ P(w_t | w_{<t}) $, we first select the top $ k $ most likely words according to this distribution, and then sample from this restricted set of words. This helps in controlling the diversity of the generated outputs while still allowing for some randomness in the sampling process.\n",
    "\n",
    "In this context, $ \\text{Top-}k\\left(P(w_t | w_{<t})\\right) $ represents the selection of the top $ k $ words with the highest probabilities from the distribution $ P(w_t | w_{<t}) $, and $ w_t $ is sampled from this restricted set of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:41:12.797535Z",
     "start_time": "2024-04-03T08:41:07.020300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: In this assignment, we will delve into different generation methods of GPT. To be specific, you will\n",
      "generated_text: In this assignment, we will delve into different generation methods of GPT. To be specific, you will need to define a set of different types of GPTs and define some types that are different from each other.\n",
      "\n",
      "In this assignment, I will be using the following methods for defining GPTs:\n",
      "\n",
      "Generate a new GPT with a new set of types (for example) and define the type\n"
     ]
    }
   ],
   "source": [
    "def sampling_generation(input_ids, model, topk) -> str:\n",
    "    outputs = input_ids\n",
    "\n",
    "    # =========================\n",
    "    # Your code starts here (3 points)\n",
    "    # outputs should be a torch Tensor of shape [1, n]\n",
    "    # where n is the length of the generated text\n",
    "    # and the elements are the token ids of the generated text\n",
    "    # \n",
    "    # You should not use any external library like transformers's .generate().\n",
    "    # Only torch is allowed.\n",
    "    # =========================\n",
    "    model.eval()\n",
    "    max_length = 64\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(outputs)[0][:, -1, :]  # Get logits for the last generated token\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "            top_probs, top_ix = torch.topk(probs, k=topk)  # Select top-k tokens based on probabilities\n",
    "            \n",
    "            # Sample from the top-k tokens. This randomness introduces diversity in the generated text\n",
    "            choices = torch.multinomial(top_probs, 1)\n",
    "            next_token_id = top_ix.gather(1, choices)  # Select the next token ID based on the sampled choice\n",
    "            \n",
    "            # Break if the end-of-sequence token is generated\n",
    "            if next_token_id[0, 0].item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            # Append the next token ID to the output sequence\n",
    "            outputs = torch.cat([outputs, next_token_id], dim=1)\n",
    "        \n",
    "    # =========================\n",
    "    # Your code ends here\n",
    "    # =========================\n",
    "\n",
    "    return outputs\n",
    "\n",
    "generated_ids = sampling_generation(input_ids, model, topk=5)[0]\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(f\"prompt: {prompt}\\ngenerated_text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
